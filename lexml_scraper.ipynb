{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nLEXML SCRAPER\\n\\nScrapes the information regarding laws available on https://www.lexml.gov.br/. \\nBy using a lexicon_flag, you can also generate an extra file with all law's subjects\\nand how frequently they appareted. (law's subjects = P1269 (facets of))\\n\\nTo scrape different types of law, change the keywords in the search URL. \\n\\nThe output files are .txt separated by *. \\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "LEXML SCRAPER\n",
    "\n",
    "Scrapes the information regarding laws available on https://www.lexml.gov.br/. \n",
    "By using a lexicon_flag, you can also generate an extra file with all law's subjects\n",
    "and how frequently they appareted. (law's subjects = P1269 (facets of))\n",
    "\n",
    "To scrape different types of law, change the keywords in the search URL. \n",
    "\n",
    "The output files are .txt separated by *. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "import requests\n",
    "import collections\n",
    "import time\n",
    "\n",
    "# Namespace\n",
    "ns = {\n",
    "    'srw_dc': 'info:srw/schema/1/dc-schema',\n",
    "    'dc'    : 'http://purl.org/dc/elements/1.1/',\n",
    "    'srw'   : 'http://www.loc.gov/zing/srw/',\n",
    "    'xsi'   : 'http://www.w3.org/2001/XMLSchema'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Law class that will store all of the scraped data.\n",
    "\"\"\"\n",
    "class law:\n",
    "    def __init__(self):  \n",
    "        self.tipoDocumento = \"\"\n",
    "        self.date = \"\"\n",
    "        self.urn = \"\"\n",
    "        self.localidade = \"\"\n",
    "        self.autoridade = \"\"\n",
    "        self.title = \"\"\n",
    "        self.description = \"\"\n",
    "        self.identifier = \"\"\n",
    "        self.subject = []\n",
    "        \n",
    "    def print_self(self):\n",
    "        return (self.tipoDocumento + \"*\" + self.date + \"*\" + self.urn + \\\n",
    "               \"*\" + self.localidade + \"*\" + self.autoridade + \"*\" + self.title + \\\n",
    "               \"*\" + self.description + \"*\" + self.identifier + \"*\" + \"*\".join(self.subject) + \"\\n\")\n",
    "\n",
    "\"\"\"\n",
    "Used for keeping track of how many times each law subject was used.\n",
    "\"\"\"\n",
    "class lexicon:\n",
    "    def __init__(self, word, count):\n",
    "        self.word = word\n",
    "        self.count = count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Receives an x item, extracts all attributes in it and saves them to a \n",
    "new_law object, which is returned.\n",
    "\"\"\"\n",
    "def get_values(x):\n",
    "    new_law = law()\n",
    "    for i in x.iter():\n",
    "        tag = i.tag\n",
    "        if (\"}\") in tag:\n",
    "            tag = tag.split(\"}\")[1]\n",
    "        if tag in attributes:\n",
    "            if (tag != \"subject\"):\n",
    "                if (i.text):\n",
    "                    i.text = i.text.replace(\"\\n\", \"\")\n",
    "                setattr(new_law, tag, i.text)\n",
    "            else:\n",
    "                subjects = [x.strip() for x in re.split('\\s[,.]\\s', i.text)]\n",
    "                unique_subjects = list(set(subjects))\n",
    "                for i in range(0, len(unique_subjects)):\n",
    "                    if (\" .\" in unique_subjects[i]):\n",
    "                        unique_subjects[i] = unique_subjects[i][:-2]\n",
    "                setattr(new_law, tag, unique_subjects)\n",
    "    return new_law\n",
    "\n",
    "\"\"\"\n",
    "Receives a list of laws. Returns a lexicon containing all of the law's\n",
    "subjects and how frequently they were used.\n",
    "Duplicate subjects within the same law are ignored. \n",
    "\"\"\"\n",
    "def get_lexicon(laws):\n",
    "    lex = []\n",
    "    for l in laws:\n",
    "        for s in l.subject:\n",
    "            lex.append(s)\n",
    "    lexicon = collections.Counter(lex)\n",
    "    lexicon = sorted(lexicon.items(), key = lambda lex: lex[1], reverse = True)\n",
    "    return lexicon\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Receives a lexicon list and prints it's content into file_name_lexicon.txt.\n",
    "\"\"\"\n",
    "def print_lexicon(lexicon, file_name):\n",
    "    file_lex = open(file_name + \"_lexicon.txt\", \"w\")\n",
    "    for key, value in lexicon:\n",
    "        file_lex.write(str(value) + \"*\" + key + \"\\n\")\n",
    "    file_lex.close()\n",
    "    \n",
    "    \n",
    "\"\"\"    \n",
    "Prints all data scraped from the laws into file_name.txt, \n",
    "separeted by *, in the format:\n",
    "Type_of_document*date*urn*locality*authority*title*description*identifier*subjects\n",
    "\"\"\"\n",
    "def print_scraped_info(laws, file_name):\n",
    "    file = open(file_name + \".txt\", \"w\")  \n",
    "    file.write(\"tipo de documento*data*urn*localidade*autoridade*\" + \\\n",
    "               \"tÃ­tulo*descricao*identifier*assuntos->\\n\")\n",
    "    for i in laws:\n",
    "        file.write(i.print_self())\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML attributes that will be extracted\n",
    "attributes = (\"tipoDocumento\", \"date\", \"urn\", \"localidade\", \"autoridade\", \"title\", \"description\", \"identifier\", \"subject\")\n",
    "\n",
    "\n",
    "# federal laws (leis federais)\n",
    "search_term = \"%22federal+decreto.lei%22\" #\"%22lei+federal%22\"\n",
    "base_url = \"https://www.lexml.gov.br/busca/SRU?operation=searchRetrieve&query=urn+=\" + search_term + \"&maximumRecords=500&startRecord=\"\n",
    "\n",
    "\n",
    "file_name = \"decretos_lei\"\n",
    "n = 1\n",
    "lexicon_flag = True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    laws = []\n",
    "    for i in range(0, n):\n",
    "        url = base_url + str(i * 500 + 1)\n",
    "        req = requests.request('GET', url)\n",
    "        tree = etree.fromstring(req.content)\n",
    "\n",
    "        # x stands for each entry in <srw_dc:dc>\n",
    "        for x in tree.findall(\".//srw_dc:dc\", namespaces=ns):\n",
    "            new_law = get_values(x)\n",
    "            laws.append(new_law)\n",
    "            \n",
    "        # Being polite\n",
    "        time.sleep(2)\n",
    "            \n",
    "    if (lexicon_flag):\n",
    "        lexicon = get_lexicon(laws)\n",
    "        print_lexicon(lexicon, file_name)\n",
    "\n",
    "    print_scraped_info(laws, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two lexicon files generated can be merged in the command line using:\n",
    "\n",
    "> sort file1.txt file2.txt | uniq > exit_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Receives a list of string with previously generated lexicon file names. \n",
    "Generates a file named merged_lexicon_files.txt in the current folder, \n",
    "with the sum of how many times each term has apparead in each lexicon file.\n",
    "\"\"\"\n",
    "def merge_lexicon_files (list_of_files):\n",
    "    lex = {}\n",
    "    for file in list_of_files:\n",
    "        f = open(file)\n",
    "        for line in f.readlines():\n",
    "            value, key = line.split(\"*\")\n",
    "            key = key.strip()\n",
    "            lex[key] = lex.get(key, 0) + int(value)\n",
    "            \n",
    "    lex = sorted(lex.items(), key = lambda l: l[1], reverse = True)\n",
    "    \n",
    "    file = open(\"merged_lexicon_files.txt\", \"w\")\n",
    "    for k, v in lex:\n",
    "        file.write(str(v) + \"*\" + k + \"\\n\")\n",
    "    file.close()\n",
    "            \n",
    "lex = merge_lexicon_files([\"decretos-lei-complete_lexicon.txt\", \"leis-complete_lexicon.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.5\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
